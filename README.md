# annotaudit

A developer-friendly Python toolkit designed to audit, cleanse, and analyze human-annotated datasets within machine learning pipelines. `annotaudit` helps ensure data quality, identify annotation inconsistencies, and provide insights into annotator performance and behavior.

## Features

-   **âš–ï¸ Detect Annotation Drift and Fatigue:** Algorithms to identify shifts in annotation patterns over time and signs of annotator fatigue.
-   **ğŸ›¡ï¸ Identify Rushed or Fabricated Task Completion:** Tools to flag tasks completed unusually quickly or exhibiting suspicious patterns.
-   **ğŸ‘ï¸ Visualize Annotator Profiles and Work Trends:** Generate visual reports on individual annotator performance, consistency, and workload distribution.
-   **ğŸ¤– Optional LLM-Assisted Relabeling:** Integrate with Large Language Models to assist in the relabeling process for improved accuracy and efficiency.
-   **ğŸ§¹ Cleansing Pipeline:** A robust pipeline to filter inconsistent or noisy labels, ensuring a high-quality dataset for model training.

## Installation

To get started with `annotaudit`, clone the repository and install the required dependencies:

```bash
git clone https://github.com/kliewerdaniel/annotaudit01.git
cd annotaudit01
pip install -r requirements.txt
```

## Usage

`annotaudit` can be run via the `main.py` script with various command-line arguments to perform different auditing and cleansing tasks.

### Basic Audit

To perform a basic audit on your annotation data:

```bash
python main.py --audit path/to/your_annotations.csv --config config/schema.yaml
```

-   `--audit`: Specifies the path to your input CSV file containing annotations.
-   `--config`: Specifies the path to your configuration schema file (e.g., `config/schema.yaml`) which defines your annotation task and label structure.

### Generating Reports

After running an audit, various reports and visualizations are generated in the `annotaudit_results/` directory.

For example, to view annotator profiles:

```bash
# This is automatically generated after an audit
open annotaudit_results/annotator_profiles.png
```

### Cleansing Labels

To apply cleansing operations to your labels:

```bash
python main.py --cleanse path/to/your_annotations.csv --config config/schema.yaml --output cleaned_labels.csv
```

-   `--cleanse`: Initiates the cleansing pipeline.
-   `--output`: Specifies the output file path for the cleaned labels.

### Configuration

The `config/schema.yaml` file is crucial for defining your annotation project's structure, including label types, relationships, and any specific rules for auditing. An example schema might look like this:

```yaml
# config/schema.yaml
dataset_schema:
  id_column: "task_id"
  annotator_column: "annotator_id"
  timestamp_column: "timestamp"
  label_columns:
    - "category"
    - "sentiment"
  label_definitions:
    category:
      type: "categorical"
      values: ["A", "B", "C"]
    sentiment:
      type: "categorical"
      values: ["positive", "negative", "neutral"]
```

## Project Structure

```
.
â”œâ”€â”€ main.py                     # Main entry point for running audits and cleansing
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ annotaudit_results/         # Directory for generated reports and visualizations
â”‚   â”œâ”€â”€ annotator_profiles.png
â”‚   â”œâ”€â”€ auto_relabel_results.csv
â”‚   â”œâ”€â”€ cleaned_labels.csv
â”‚   â”œâ”€â”€ consistency_scores.csv
â”‚   â”œâ”€â”€ drift_scores.csv
â”‚   â”œâ”€â”€ flagged_fast_tasks.csv
â”‚   â”œâ”€â”€ resolved_labels.csv
â”‚   â”œâ”€â”€ timeline_stats.png
â”‚   â”œâ”€â”€ wage_efficiency_report.csv
â”‚   â””â”€â”€ workload_summary.csv
â”œâ”€â”€ audit/                      # Modules for auditing annotation quality
â”‚   â”œâ”€â”€ consistency.py
â”‚   â”œâ”€â”€ drift.py
â”‚   â”œâ”€â”€ redundancy_check.py
â”‚   â””â”€â”€ speed_check.py
â”œâ”€â”€ cleansing/                  # Modules for cleansing and relabeling
â”‚   â”œâ”€â”€ auto_relabel.py
â”‚   â””â”€â”€ label_filter.py
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ schema.yaml             # Example schema for dataset definition
â”œâ”€â”€ ethics/                     # Modules for ethical considerations in annotation
â”‚   â”œâ”€â”€ wage_efficiency.py
â”‚   â””â”€â”€ workload_analysis.py
â”œâ”€â”€ examples/                   # Example usage and sample data
â”‚   â”œâ”€â”€ audit_example.ipynb
â”‚   â””â”€â”€ sample_data.csv
â”œâ”€â”€ tests/                      # Unit and integration tests
â”‚   â””â”€â”€ test_consistency.py
â””â”€â”€ viz/                        # Visualization scripts
    â”œâ”€â”€ annotator_profiles.py
    â””â”€â”€ timeline_stats.py
```

